{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21077 images belonging to 4 classes.\n",
      "Found 5140 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import TopKCategoricalAccuracy, CategoricalAccuracy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "\n",
    "# Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    r\"D:\\code\\archive\\images2\\images\\train\",\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode = 'rgb',\n",
    "    shuffle=True,\n",
    "    seed=99\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_dataset = val_datagen.flow_from_directory(\n",
    "    r\"D:\\code\\archive\\images2\\images\\validation\",\n",
    "    target_size=(48, 48),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    color_mode = 'rgb',\n",
    "    shuffle=True,\n",
    "    seed=99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet50 model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "x = tf.keras.layers.Dense(16, activation='relu')(x)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)  # Add dropout layer\n",
    "output = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Fine-tune some layers of ResNet50\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Compile the model\n",
    "loss_function = CategoricalCrossentropy()\n",
    "metrics = [CategoricalAccuracy(name=\"accuracy\"), TopKCategoricalAccuracy(k=4, name=\"top_k_accuracy\")]\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),#1e-4 77% 47%\n",
    "    loss=loss_function,\n",
    "    metrics=metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659/659 [==============================] - 1392s 2s/step - loss: 1.3590 - accuracy: 0.3636 - top_k_accuracy: 1.0000 - val_loss: 2.1731 - val_accuracy: 0.3364 - val_top_k_accuracy: 1.0000\n",
      "Epoch 2/120\n",
      "659/659 [==============================] - 1496s 2s/step - loss: 1.1105 - accuracy: 0.5200 - top_k_accuracy: 1.0000 - val_loss: 0.9610 - val_accuracy: 0.6072 - val_top_k_accuracy: 1.0000\n",
      "Epoch 3/120\n",
      "659/659 [==============================] - 633s 959ms/step - loss: 0.9806 - accuracy: 0.5891 - top_k_accuracy: 1.0000 - val_loss: 0.8699 - val_accuracy: 0.6420 - val_top_k_accuracy: 1.0000\n",
      "Epoch 4/120\n",
      "659/659 [==============================] - 583s 884ms/step - loss: 0.9215 - accuracy: 0.6217 - top_k_accuracy: 1.0000 - val_loss: 0.8309 - val_accuracy: 0.6626 - val_top_k_accuracy: 1.0000\n",
      "Epoch 5/120\n",
      "659/659 [==============================] - 584s 886ms/step - loss: 0.8654 - accuracy: 0.6439 - top_k_accuracy: 1.0000 - val_loss: 0.8190 - val_accuracy: 0.6568 - val_top_k_accuracy: 1.0000\n",
      "Epoch 6/120\n",
      "659/659 [==============================] - 583s 885ms/step - loss: 0.8468 - accuracy: 0.6568 - top_k_accuracy: 1.0000 - val_loss: 0.8114 - val_accuracy: 0.6665 - val_top_k_accuracy: 1.0000\n",
      "Epoch 7/120\n",
      "659/659 [==============================] - 585s 887ms/step - loss: 0.8209 - accuracy: 0.6662 - top_k_accuracy: 1.0000 - val_loss: 0.7818 - val_accuracy: 0.6825 - val_top_k_accuracy: 1.0000\n",
      "Epoch 8/120\n",
      "659/659 [==============================] - 588s 893ms/step - loss: 0.7954 - accuracy: 0.6771 - top_k_accuracy: 1.0000 - val_loss: 0.7726 - val_accuracy: 0.6891 - val_top_k_accuracy: 1.0000\n",
      "Epoch 9/120\n",
      "659/659 [==============================] - 584s 886ms/step - loss: 0.7836 - accuracy: 0.6876 - top_k_accuracy: 1.0000 - val_loss: 0.7523 - val_accuracy: 0.6986 - val_top_k_accuracy: 1.0000\n",
      "Epoch 10/120\n",
      "659/659 [==============================] - 582s 883ms/step - loss: 0.7705 - accuracy: 0.6877 - top_k_accuracy: 1.0000 - val_loss: 0.7554 - val_accuracy: 0.6990 - val_top_k_accuracy: 1.0000\n",
      "Epoch 11/120\n",
      "659/659 [==============================] - 581s 882ms/step - loss: 0.7554 - accuracy: 0.6980 - top_k_accuracy: 1.0000 - val_loss: 0.7432 - val_accuracy: 0.7058 - val_top_k_accuracy: 1.0000\n",
      "Epoch 12/120\n",
      "659/659 [==============================] - 582s 883ms/step - loss: 0.7306 - accuracy: 0.7032 - top_k_accuracy: 1.0000 - val_loss: 0.8081 - val_accuracy: 0.6856 - val_top_k_accuracy: 1.0000\n",
      "Epoch 13/120\n",
      "659/659 [==============================] - 572s 868ms/step - loss: 0.7141 - accuracy: 0.7126 - top_k_accuracy: 1.0000 - val_loss: 0.7143 - val_accuracy: 0.7125 - val_top_k_accuracy: 1.0000\n",
      "Epoch 14/120\n",
      "659/659 [==============================] - 578s 877ms/step - loss: 0.6884 - accuracy: 0.7233 - top_k_accuracy: 1.0000 - val_loss: 0.7124 - val_accuracy: 0.7095 - val_top_k_accuracy: 1.0000\n",
      "Epoch 15/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.6807 - accuracy: 0.7280 - top_k_accuracy: 1.0000 - val_loss: 0.7101 - val_accuracy: 0.7198 - val_top_k_accuracy: 1.0000\n",
      "Epoch 16/120\n",
      "659/659 [==============================] - 587s 890ms/step - loss: 0.6660 - accuracy: 0.7269 - top_k_accuracy: 1.0000 - val_loss: 0.7098 - val_accuracy: 0.7241 - val_top_k_accuracy: 1.0000\n",
      "Epoch 17/120\n",
      "659/659 [==============================] - 584s 885ms/step - loss: 0.6491 - accuracy: 0.7427 - top_k_accuracy: 1.0000 - val_loss: 0.6901 - val_accuracy: 0.7383 - val_top_k_accuracy: 1.0000\n",
      "Epoch 18/120\n",
      "659/659 [==============================] - 585s 888ms/step - loss: 0.6310 - accuracy: 0.7472 - top_k_accuracy: 1.0000 - val_loss: 0.6937 - val_accuracy: 0.7292 - val_top_k_accuracy: 1.0000\n",
      "Epoch 19/120\n",
      "659/659 [==============================] - 584s 887ms/step - loss: 0.6191 - accuracy: 0.7518 - top_k_accuracy: 1.0000 - val_loss: 0.6990 - val_accuracy: 0.7274 - val_top_k_accuracy: 1.0000\n",
      "Epoch 20/120\n",
      "659/659 [==============================] - 577s 876ms/step - loss: 0.6162 - accuracy: 0.7536 - top_k_accuracy: 1.0000 - val_loss: 0.7195 - val_accuracy: 0.7189 - val_top_k_accuracy: 1.0000\n",
      "Epoch 21/120\n",
      "659/659 [==============================] - 582s 883ms/step - loss: 0.5991 - accuracy: 0.7631 - top_k_accuracy: 1.0000 - val_loss: 0.7197 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 22/120\n",
      "659/659 [==============================] - 583s 885ms/step - loss: 0.5779 - accuracy: 0.7691 - top_k_accuracy: 1.0000 - val_loss: 0.7255 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 23/120\n",
      "659/659 [==============================] - 584s 886ms/step - loss: 0.5694 - accuracy: 0.7734 - top_k_accuracy: 1.0000 - val_loss: 0.7146 - val_accuracy: 0.7243 - val_top_k_accuracy: 1.0000\n",
      "Epoch 24/120\n",
      "659/659 [==============================] - 588s 892ms/step - loss: 0.5557 - accuracy: 0.7860 - top_k_accuracy: 1.0000 - val_loss: 0.7163 - val_accuracy: 0.7241 - val_top_k_accuracy: 1.0000\n",
      "Epoch 25/120\n",
      "659/659 [==============================] - 591s 896ms/step - loss: 0.5528 - accuracy: 0.7786 - top_k_accuracy: 1.0000 - val_loss: 0.7195 - val_accuracy: 0.7232 - val_top_k_accuracy: 1.0000\n",
      "Epoch 26/120\n",
      "659/659 [==============================] - 591s 898ms/step - loss: 0.5540 - accuracy: 0.7792 - top_k_accuracy: 1.0000 - val_loss: 0.7213 - val_accuracy: 0.7226 - val_top_k_accuracy: 1.0000\n",
      "Epoch 27/120\n",
      "659/659 [==============================] - 590s 895ms/step - loss: 0.5464 - accuracy: 0.7845 - top_k_accuracy: 1.0000 - val_loss: 0.7145 - val_accuracy: 0.7282 - val_top_k_accuracy: 1.0000\n",
      "Epoch 28/120\n",
      "659/659 [==============================] - 590s 895ms/step - loss: 0.5485 - accuracy: 0.7792 - top_k_accuracy: 1.0000 - val_loss: 0.7215 - val_accuracy: 0.7259 - val_top_k_accuracy: 1.0000\n",
      "Epoch 29/120\n",
      "659/659 [==============================] - 591s 897ms/step - loss: 0.5332 - accuracy: 0.7892 - top_k_accuracy: 1.0000 - val_loss: 0.7238 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 30/120\n",
      "659/659 [==============================] - 598s 907ms/step - loss: 0.5378 - accuracy: 0.7878 - top_k_accuracy: 1.0000 - val_loss: 0.7241 - val_accuracy: 0.7249 - val_top_k_accuracy: 1.0000\n",
      "Epoch 31/120\n",
      "659/659 [==============================] - 574s 870ms/step - loss: 0.5393 - accuracy: 0.7854 - top_k_accuracy: 1.0000 - val_loss: 0.7244 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 32/120\n",
      "659/659 [==============================] - 558s 847ms/step - loss: 0.5467 - accuracy: 0.7803 - top_k_accuracy: 1.0000 - val_loss: 0.7205 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 33/120\n",
      "659/659 [==============================] - 577s 876ms/step - loss: 0.5416 - accuracy: 0.7827 - top_k_accuracy: 1.0000 - val_loss: 0.7249 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 34/120\n",
      "659/659 [==============================] - 581s 882ms/step - loss: 0.5497 - accuracy: 0.7840 - top_k_accuracy: 1.0000 - val_loss: 0.7232 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 35/120\n",
      "659/659 [==============================] - 584s 887ms/step - loss: 0.5437 - accuracy: 0.7840 - top_k_accuracy: 1.0000 - val_loss: 0.7237 - val_accuracy: 0.7270 - val_top_k_accuracy: 1.0000\n",
      "Epoch 36/120\n",
      "659/659 [==============================] - 585s 887ms/step - loss: 0.5334 - accuracy: 0.7890 - top_k_accuracy: 1.0000 - val_loss: 0.7239 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 37/120\n",
      "659/659 [==============================] - 583s 885ms/step - loss: 0.5454 - accuracy: 0.7813 - top_k_accuracy: 1.0000 - val_loss: 0.7225 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 38/120\n",
      "659/659 [==============================] - 582s 884ms/step - loss: 0.5463 - accuracy: 0.7805 - top_k_accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 0.7265 - val_top_k_accuracy: 1.0000\n",
      "Epoch 39/120\n",
      "659/659 [==============================] - 580s 880ms/step - loss: 0.5379 - accuracy: 0.7846 - top_k_accuracy: 1.0000 - val_loss: 0.7239 - val_accuracy: 0.7265 - val_top_k_accuracy: 1.0000\n",
      "Epoch 40/120\n",
      "659/659 [==============================] - 580s 881ms/step - loss: 0.5329 - accuracy: 0.7892 - top_k_accuracy: 1.0000 - val_loss: 0.7274 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 41/120\n",
      "659/659 [==============================] - 589s 894ms/step - loss: 0.5364 - accuracy: 0.7882 - top_k_accuracy: 1.0000 - val_loss: 0.7231 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 42/120\n",
      "659/659 [==============================] - 588s 891ms/step - loss: 0.5321 - accuracy: 0.7903 - top_k_accuracy: 1.0000 - val_loss: 0.7234 - val_accuracy: 0.7259 - val_top_k_accuracy: 1.0000\n",
      "Epoch 43/120\n",
      "659/659 [==============================] - 587s 891ms/step - loss: 0.5455 - accuracy: 0.7810 - top_k_accuracy: 1.0000 - val_loss: 0.7251 - val_accuracy: 0.7249 - val_top_k_accuracy: 1.0000\n",
      "Epoch 44/120\n",
      "659/659 [==============================] - 595s 903ms/step - loss: 0.5548 - accuracy: 0.7759 - top_k_accuracy: 1.0000 - val_loss: 0.7231 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 45/120\n",
      "659/659 [==============================] - 571s 867ms/step - loss: 0.5422 - accuracy: 0.7860 - top_k_accuracy: 1.0000 - val_loss: 0.7250 - val_accuracy: 0.7251 - val_top_k_accuracy: 1.0000\n",
      "Epoch 46/120\n",
      "659/659 [==============================] - 567s 861ms/step - loss: 0.5428 - accuracy: 0.7852 - top_k_accuracy: 1.0000 - val_loss: 0.7245 - val_accuracy: 0.7272 - val_top_k_accuracy: 1.0000\n",
      "Epoch 47/120\n",
      "659/659 [==============================] - 577s 875ms/step - loss: 0.5419 - accuracy: 0.7838 - top_k_accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 48/120\n",
      "659/659 [==============================] - 571s 867ms/step - loss: 0.5373 - accuracy: 0.7867 - top_k_accuracy: 1.0000 - val_loss: 0.7215 - val_accuracy: 0.7272 - val_top_k_accuracy: 1.0000\n",
      "Epoch 49/120\n",
      "659/659 [==============================] - 582s 883ms/step - loss: 0.5415 - accuracy: 0.7819 - top_k_accuracy: 1.0000 - val_loss: 0.7246 - val_accuracy: 0.7253 - val_top_k_accuracy: 1.0000\n",
      "Epoch 50/120\n",
      "659/659 [==============================] - 580s 880ms/step - loss: 0.5395 - accuracy: 0.7857 - top_k_accuracy: 1.0000 - val_loss: 0.7239 - val_accuracy: 0.7265 - val_top_k_accuracy: 1.0000\n",
      "Epoch 51/120\n",
      "659/659 [==============================] - 581s 882ms/step - loss: 0.5429 - accuracy: 0.7856 - top_k_accuracy: 1.0000 - val_loss: 0.7236 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 52/120\n",
      "659/659 [==============================] - 587s 890ms/step - loss: 0.5434 - accuracy: 0.7783 - top_k_accuracy: 1.0000 - val_loss: 0.7237 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 53/120\n",
      "659/659 [==============================] - 585s 888ms/step - loss: 0.5334 - accuracy: 0.7852 - top_k_accuracy: 1.0000 - val_loss: 0.7235 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 54/120\n",
      "659/659 [==============================] - 586s 890ms/step - loss: 0.5340 - accuracy: 0.7887 - top_k_accuracy: 1.0000 - val_loss: 0.7242 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 55/120\n",
      "659/659 [==============================] - 587s 891ms/step - loss: 0.5514 - accuracy: 0.7790 - top_k_accuracy: 1.0000 - val_loss: 0.7233 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 56/120\n",
      "659/659 [==============================] - 588s 893ms/step - loss: 0.5430 - accuracy: 0.7844 - top_k_accuracy: 1.0000 - val_loss: 0.7228 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 57/120\n",
      "659/659 [==============================] - 586s 890ms/step - loss: 0.5387 - accuracy: 0.7827 - top_k_accuracy: 1.0000 - val_loss: 0.7217 - val_accuracy: 0.7270 - val_top_k_accuracy: 1.0000\n",
      "Epoch 58/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5328 - accuracy: 0.7845 - top_k_accuracy: 1.0000 - val_loss: 0.7231 - val_accuracy: 0.7265 - val_top_k_accuracy: 1.0000\n",
      "Epoch 59/120\n",
      "659/659 [==============================] - 592s 899ms/step - loss: 0.5508 - accuracy: 0.7826 - top_k_accuracy: 1.0000 - val_loss: 0.7196 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 60/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5437 - accuracy: 0.7828 - top_k_accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 61/120\n",
      "659/659 [==============================] - 584s 886ms/step - loss: 0.5347 - accuracy: 0.7876 - top_k_accuracy: 1.0000 - val_loss: 0.7264 - val_accuracy: 0.7251 - val_top_k_accuracy: 1.0000\n",
      "Epoch 62/120\n",
      "659/659 [==============================] - 587s 891ms/step - loss: 0.5486 - accuracy: 0.7793 - top_k_accuracy: 1.0000 - val_loss: 0.7231 - val_accuracy: 0.7251 - val_top_k_accuracy: 1.0000\n",
      "Epoch 63/120\n",
      "659/659 [==============================] - 588s 893ms/step - loss: 0.5412 - accuracy: 0.7817 - top_k_accuracy: 1.0000 - val_loss: 0.7247 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 64/120\n",
      "659/659 [==============================] - 576s 875ms/step - loss: 0.5497 - accuracy: 0.7840 - top_k_accuracy: 1.0000 - val_loss: 0.7219 - val_accuracy: 0.7276 - val_top_k_accuracy: 1.0000\n",
      "Epoch 65/120\n",
      "659/659 [==============================] - 582s 883ms/step - loss: 0.5377 - accuracy: 0.7872 - top_k_accuracy: 1.0000 - val_loss: 0.7232 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 66/120\n",
      "659/659 [==============================] - 591s 897ms/step - loss: 0.5332 - accuracy: 0.7871 - top_k_accuracy: 1.0000 - val_loss: 0.7243 - val_accuracy: 0.7247 - val_top_k_accuracy: 1.0000\n",
      "Epoch 67/120\n",
      "659/659 [==============================] - 584s 887ms/step - loss: 0.5510 - accuracy: 0.7819 - top_k_accuracy: 1.0000 - val_loss: 0.7214 - val_accuracy: 0.7268 - val_top_k_accuracy: 1.0000\n",
      "Epoch 68/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5395 - accuracy: 0.7850 - top_k_accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 69/120\n",
      "659/659 [==============================] - 588s 892ms/step - loss: 0.5419 - accuracy: 0.7826 - top_k_accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 70/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5299 - accuracy: 0.7880 - top_k_accuracy: 1.0000 - val_loss: 0.7252 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 71/120\n",
      "659/659 [==============================] - 584s 886ms/step - loss: 0.5383 - accuracy: 0.7843 - top_k_accuracy: 1.0000 - val_loss: 0.7215 - val_accuracy: 0.7274 - val_top_k_accuracy: 1.0000\n",
      "Epoch 72/120\n",
      "659/659 [==============================] - 585s 888ms/step - loss: 0.5338 - accuracy: 0.7919 - top_k_accuracy: 1.0000 - val_loss: 0.7239 - val_accuracy: 0.7259 - val_top_k_accuracy: 1.0000\n",
      "Epoch 73/120\n",
      "659/659 [==============================] - 587s 891ms/step - loss: 0.5548 - accuracy: 0.7778 - top_k_accuracy: 1.0000 - val_loss: 0.7229 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 74/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5318 - accuracy: 0.7887 - top_k_accuracy: 1.0000 - val_loss: 0.7201 - val_accuracy: 0.7272 - val_top_k_accuracy: 1.0000\n",
      "Epoch 75/120\n",
      "659/659 [==============================] - 588s 893ms/step - loss: 0.5430 - accuracy: 0.7838 - top_k_accuracy: 1.0000 - val_loss: 0.7266 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 76/120\n",
      "659/659 [==============================] - 592s 899ms/step - loss: 0.5452 - accuracy: 0.7828 - top_k_accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 0.7274 - val_top_k_accuracy: 1.0000\n",
      "Epoch 77/120\n",
      "659/659 [==============================] - 583s 885ms/step - loss: 0.5484 - accuracy: 0.7825 - top_k_accuracy: 1.0000 - val_loss: 0.7247 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 78/120\n",
      "659/659 [==============================] - 587s 890ms/step - loss: 0.5426 - accuracy: 0.7849 - top_k_accuracy: 1.0000 - val_loss: 0.7247 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 79/120\n",
      "659/659 [==============================] - 584s 887ms/step - loss: 0.5417 - accuracy: 0.7880 - top_k_accuracy: 1.0000 - val_loss: 0.7240 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 80/120\n",
      "659/659 [==============================] - 586s 890ms/step - loss: 0.5369 - accuracy: 0.7868 - top_k_accuracy: 1.0000 - val_loss: 0.7220 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 81/120\n",
      "659/659 [==============================] - 587s 891ms/step - loss: 0.5439 - accuracy: 0.7780 - top_k_accuracy: 1.0000 - val_loss: 0.7224 - val_accuracy: 0.7259 - val_top_k_accuracy: 1.0000\n",
      "Epoch 82/120\n",
      "659/659 [==============================] - 589s 894ms/step - loss: 0.5389 - accuracy: 0.7836 - top_k_accuracy: 1.0000 - val_loss: 0.7219 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 83/120\n",
      "659/659 [==============================] - 572s 869ms/step - loss: 0.5365 - accuracy: 0.7883 - top_k_accuracy: 1.0000 - val_loss: 0.7219 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 84/120\n",
      "659/659 [==============================] - 585s 887ms/step - loss: 0.5415 - accuracy: 0.7824 - top_k_accuracy: 1.0000 - val_loss: 0.7209 - val_accuracy: 0.7268 - val_top_k_accuracy: 1.0000\n",
      "Epoch 85/120\n",
      "659/659 [==============================] - 585s 888ms/step - loss: 0.5470 - accuracy: 0.7806 - top_k_accuracy: 1.0000 - val_loss: 0.7234 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 86/120\n",
      "659/659 [==============================] - 588s 893ms/step - loss: 0.5334 - accuracy: 0.7861 - top_k_accuracy: 1.0000 - val_loss: 0.7248 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 87/120\n",
      "659/659 [==============================] - 586s 890ms/step - loss: 0.5416 - accuracy: 0.7844 - top_k_accuracy: 1.0000 - val_loss: 0.7226 - val_accuracy: 0.7274 - val_top_k_accuracy: 1.0000\n",
      "Epoch 88/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5312 - accuracy: 0.7842 - top_k_accuracy: 1.0000 - val_loss: 0.7218 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 89/120\n",
      "659/659 [==============================] - 583s 885ms/step - loss: 0.5318 - accuracy: 0.7902 - top_k_accuracy: 1.0000 - val_loss: 0.7226 - val_accuracy: 0.7263 - val_top_k_accuracy: 1.0000\n",
      "Epoch 90/120\n",
      "659/659 [==============================] - 587s 890ms/step - loss: 0.5513 - accuracy: 0.7802 - top_k_accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 0.7265 - val_top_k_accuracy: 1.0000\n",
      "Epoch 91/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5542 - accuracy: 0.7821 - top_k_accuracy: 1.0000 - val_loss: 0.7256 - val_accuracy: 0.7272 - val_top_k_accuracy: 1.0000\n",
      "Epoch 92/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5387 - accuracy: 0.7885 - top_k_accuracy: 1.0000 - val_loss: 0.7244 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 93/120\n",
      "659/659 [==============================] - 588s 892ms/step - loss: 0.5449 - accuracy: 0.7827 - top_k_accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.7268 - val_top_k_accuracy: 1.0000\n",
      "Epoch 94/120\n",
      "659/659 [==============================] - 584s 887ms/step - loss: 0.5465 - accuracy: 0.7811 - top_k_accuracy: 1.0000 - val_loss: 0.7220 - val_accuracy: 0.7268 - val_top_k_accuracy: 1.0000\n",
      "Epoch 95/120\n",
      "659/659 [==============================] - 583s 885ms/step - loss: 0.5459 - accuracy: 0.7827 - top_k_accuracy: 1.0000 - val_loss: 0.7234 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 96/120\n",
      "659/659 [==============================] - 584s 887ms/step - loss: 0.5379 - accuracy: 0.7862 - top_k_accuracy: 1.0000 - val_loss: 0.7232 - val_accuracy: 0.7268 - val_top_k_accuracy: 1.0000\n",
      "Epoch 97/120\n",
      "659/659 [==============================] - 584s 886ms/step - loss: 0.5454 - accuracy: 0.7835 - top_k_accuracy: 1.0000 - val_loss: 0.7220 - val_accuracy: 0.7268 - val_top_k_accuracy: 1.0000\n",
      "Epoch 98/120\n",
      "659/659 [==============================] - 580s 880ms/step - loss: 0.5372 - accuracy: 0.7874 - top_k_accuracy: 1.0000 - val_loss: 0.7239 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 99/120\n",
      "659/659 [==============================] - 565s 857ms/step - loss: 0.5387 - accuracy: 0.7874 - top_k_accuracy: 1.0000 - val_loss: 0.7215 - val_accuracy: 0.7255 - val_top_k_accuracy: 1.0000\n",
      "Epoch 100/120\n",
      "659/659 [==============================] - 570s 865ms/step - loss: 0.5419 - accuracy: 0.7854 - top_k_accuracy: 1.0000 - val_loss: 0.7245 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 101/120\n",
      "659/659 [==============================] - 586s 889ms/step - loss: 0.5453 - accuracy: 0.7833 - top_k_accuracy: 1.0000 - val_loss: 0.7225 - val_accuracy: 0.7249 - val_top_k_accuracy: 1.0000\n",
      "Epoch 102/120\n",
      "659/659 [==============================] - 583s 885ms/step - loss: 0.5465 - accuracy: 0.7814 - top_k_accuracy: 1.0000 - val_loss: 0.7232 - val_accuracy: 0.7249 - val_top_k_accuracy: 1.0000\n",
      "Epoch 103/120\n",
      "659/659 [==============================] - 581s 882ms/step - loss: 0.5472 - accuracy: 0.7857 - top_k_accuracy: 1.0000 - val_loss: 0.7212 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 104/120\n",
      "659/659 [==============================] - 580s 880ms/step - loss: 0.5391 - accuracy: 0.7796 - top_k_accuracy: 1.0000 - val_loss: 0.7257 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 105/120\n",
      "659/659 [==============================] - 585s 887ms/step - loss: 0.5435 - accuracy: 0.7843 - top_k_accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 0.7267 - val_top_k_accuracy: 1.0000\n",
      "Epoch 106/120\n",
      "659/659 [==============================] - 581s 881ms/step - loss: 0.5348 - accuracy: 0.7902 - top_k_accuracy: 1.0000 - val_loss: 0.7236 - val_accuracy: 0.7247 - val_top_k_accuracy: 1.0000\n",
      "Epoch 107/120\n",
      "659/659 [==============================] - 582s 883ms/step - loss: 0.5426 - accuracy: 0.7795 - top_k_accuracy: 1.0000 - val_loss: 0.7219 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 108/120\n",
      "659/659 [==============================] - 568s 863ms/step - loss: 0.5399 - accuracy: 0.7897 - top_k_accuracy: 1.0000 - val_loss: 0.7232 - val_accuracy: 0.7268 - val_top_k_accuracy: 1.0000\n",
      "Epoch 109/120\n",
      "659/659 [==============================] - 576s 874ms/step - loss: 0.5346 - accuracy: 0.7842 - top_k_accuracy: 1.0000 - val_loss: 0.7231 - val_accuracy: 0.7276 - val_top_k_accuracy: 1.0000\n",
      "Epoch 110/120\n",
      "659/659 [==============================] - 576s 874ms/step - loss: 0.5326 - accuracy: 0.7864 - top_k_accuracy: 1.0000 - val_loss: 0.7230 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 111/120\n",
      "659/659 [==============================] - 577s 875ms/step - loss: 0.5351 - accuracy: 0.7844 - top_k_accuracy: 1.0000 - val_loss: 0.7215 - val_accuracy: 0.7265 - val_top_k_accuracy: 1.0000\n",
      "Epoch 112/120\n",
      "659/659 [==============================] - 577s 876ms/step - loss: 0.5378 - accuracy: 0.7873 - top_k_accuracy: 1.0000 - val_loss: 0.7231 - val_accuracy: 0.7257 - val_top_k_accuracy: 1.0000\n",
      "Epoch 113/120\n",
      "659/659 [==============================] - 574s 872ms/step - loss: 0.5313 - accuracy: 0.7880 - top_k_accuracy: 1.0000 - val_loss: 0.7236 - val_accuracy: 0.7265 - val_top_k_accuracy: 1.0000\n",
      "Epoch 114/120\n",
      "659/659 [==============================] - 574s 871ms/step - loss: 0.5384 - accuracy: 0.7897 - top_k_accuracy: 1.0000 - val_loss: 0.7213 - val_accuracy: 0.7261 - val_top_k_accuracy: 1.0000\n",
      "Epoch 115/120\n",
      "659/659 [==============================] - 568s 863ms/step - loss: 0.5516 - accuracy: 0.7800 - top_k_accuracy: 1.0000 - val_loss: 0.7218 - val_accuracy: 0.7251 - val_top_k_accuracy: 1.0000\n",
      "Epoch 116/120\n",
      "659/659 [==============================] - 571s 867ms/step - loss: 0.5354 - accuracy: 0.7871 - top_k_accuracy: 1.0000 - val_loss: 0.7236 - val_accuracy: 0.7247 - val_top_k_accuracy: 1.0000\n",
      "Epoch 117/120\n",
      "659/659 [==============================] - 569s 863ms/step - loss: 0.5467 - accuracy: 0.7856 - top_k_accuracy: 1.0000 - val_loss: 0.7260 - val_accuracy: 0.7245 - val_top_k_accuracy: 1.0000\n",
      "Epoch 118/120\n",
      "659/659 [==============================] - 574s 872ms/step - loss: 0.5426 - accuracy: 0.7839 - top_k_accuracy: 1.0000 - val_loss: 0.7231 - val_accuracy: 0.7268 - val_top_k_accuracy: 1.0000\n",
      "Epoch 119/120\n",
      "659/659 [==============================] - 572s 868ms/step - loss: 0.5422 - accuracy: 0.7834 - top_k_accuracy: 1.0000 - val_loss: 0.7236 - val_accuracy: 0.7253 - val_top_k_accuracy: 1.0000\n",
      "Epoch 120/120\n",
      "659/659 [==============================] - 572s 867ms/step - loss: 0.5321 - accuracy: 0.7886 - top_k_accuracy: 1.0000 - val_loss: 0.7258 - val_accuracy: 0.7259 - val_top_k_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=2e-9)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(scheduler)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=120,\n",
    "    verbose=1,\n",
    "    # callbacks=[early_stopping, reduce_lr, lr_scheduler]\n",
    "    callbacks=[reduce_lr, lr_scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('72_accuracy_model_resnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('72_accuracy_model_resnet.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the image\n",
    "img_path = r\"D:\\code\\archive\\images2\\images\\validation\\neutral\\112.jpg\"\n",
    "img = image.load_img(img_path, target_size=(48, 48))  # Resize to (48, 48)\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Normalize the pixel values (VGG16 was trained with values ranging from 0 to 255, so we normalize by dividing by 255)\n",
    "img_array /= 255.0\n",
    "\n",
    "# Expand dimensions to match the input shape required by the model (1, 48, 48, 3)\n",
    "img_array = np.expand_dims(img_array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry: 43.18%\n",
      "happy: 0.31%\n",
      "neutral: 47.46%\n",
      "sad: 9.04%\n",
      "\n",
      "Predicted emotion: neutral (47.46%)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Emotion labels used in the model\n",
    "emotion_labels = [\"angry\", \"happy\", \"neutral\", \"sad\"]\n",
    "\n",
    "# Predict the emotion\n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# Show confidence levels for each emotion\n",
    "for i, emotion in enumerate(emotion_labels):\n",
    "    confidence = predictions[0][i] * 100  # Convert to percentage\n",
    "    print(f'{emotion}: {confidence:.2f}%')\n",
    "\n",
    "# Get the index of the highest probability (most confident prediction)\n",
    "predicted_emotion_index = np.argmax(predictions)\n",
    "predicted_emotion = emotion_labels[predicted_emotion_index]\n",
    "\n",
    "# Show the predicted emotion and its confidence level\n",
    "predicted_confidence = predictions[0][predicted_emotion_index] * 100\n",
    "print(f'\\nPredicted emotion: {predicted_emotion} ({predicted_confidence:.2f}%)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
